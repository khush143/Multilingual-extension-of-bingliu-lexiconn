# -*- coding: utf-8 -*-
"""wordvec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EYb3BUM4NsKYnbu-1qTWj_-tQrY5qFw7
"""

!pip install nltk #installing the nltk package

#making the imports
import pandas as pd
import numpy as np
import multiprocessing
import re
from textblob import TextBlob
from collections import Counter
import warnings 
import seaborn as sns 
import matplotlib.pyplot as plt  

from wordcloud import WordCloud
import pickle
from nltk.stem import WordNetLemmatizer
import spacy
import nltk
nltk.download()

nltk.download('punkt')

import string#importing string

#mounting the drive
from google.colab import drive

drive.mount('/content/gdrive')

#importing nltk and re
import nltk
import re

#importing sent_tokenize and word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

#Reading the bingliu csv file
Bingliu = pd.read_csv('/content/gdrive/My Drive/NLP_ASSIGNMENT4/BingLiu.csv',sep="\t",header=None)
Bingliu.columns=["word","polarity"]
Bingliu

#opening the english.txt file and  reading it
englishcorp = open("english.txt", "r")
english=englishcorp.read()

#displaying the content of the english.txt file read
english

#reading the english_hindi-dictionary
df2 = pd.read_csv('english-hindi-dictionary.txt', sep=" ", header=None, names=["eng","pro", "hindi"])

df3=df2[["eng","hindi"]]
df3

L1=pd.DataFrame(columns=["english_word","hindi_word","polarity"])#creating a dataframe L1 with columns provided
#L1.columns=["english word","hindi word","polarity"]
L1

for i in range(0,len(Bingliu)):#lowering the words present in the bingliu
  Bingliu.loc[i,"word"].lower()

for j in range(0,len(df3)):#lowering the words present in df3
  df3.loc[i,"eng"].lower()

count=0#This is just for checking purpose to see how any words of bingliu are there in the dictionary
for i in range(0,len(Bingliu)):
  for j in range(0,len(df3)):
    if((Bingliu.loc[i,"word"])==(df3.loc[j,"eng"])):
      count=count+1
print(count)

#This step creates L1.
for i in range(0,len(Bingliu)):
  for j in range(0,len(df3)):
    if((Bingliu.loc[i,"word"])==(df3.loc[j,"eng"])):
      #L1["english word"]=Bingliu.loc[i,"word"]
      #L1["hindi word"]=df3.loc[j,"hindi"]
      #L1["polarity"]=Bingliu.loc[i,"polarity"]
      new_row = {'english_word':Bingliu.loc[i,"word"], 'hindi_word':df3.loc[j,"hindi"], 'polarity':Bingliu.loc[i,"polarity"]}
     
      L1 = L1.append(new_row, ignore_index=True)#APPENDING ROWS TO THE DATAFRAME

#THIS function is for preprocessing in the hindi corpus
def textPreHin(text):
    no_punct = "".join([i for i in text if i not in string.punctuation]) #removing punctuation
    no_numbers="".join([i for i in no_punct if i not in "0123456789"])
    hindi_actual=re.sub(r'[a-zA-z0-9 ]+'," ",no_numbers)
    
    tokens = word_tokenize(hindi_actual) 
    return(tokens)

#reading the hindi.txt file
hin_corpus = pd.read_csv (r'/content/gdrive/My Drive/NLP_ASSIGNMENT4/hindi.txt', sep = "\n",encoding="utf8",header = None)
hin_corpus.columns = ['sentences']
hin_corpus['clean text'] = hin_corpus['sentences'].apply(lambda t: textPreHin(t))#calling thr textPreHin
df = hin_corpus['clean text']
hin_vect = df.values.tolist()#converting dataframe to list

hin_vect[1:5]

#FUNCTION FOR REMOVING THE PUNCTUATIONS
def punctuations(text):
  
    regex_pattern = re.compile(r'[\,+,\:\?\!\"\(\)|!\'\\%\[\]]+')
    clean_text = regex_pattern.sub(r' ', text)
    clean_text = clean_text.replace('-', '')
    return clean_text
m1=punctuations(english)

m1

#function removing the alphanumerics
def remove_alphanumerics(text):
   
    txt = []
    for each in text.split():
        if not any(x in each.lower() for x in "0123456789"):
            txt.append(each)
    txtsent = " ".join(txt)
    return txtsent 
z=remove_alphanumerics(m1)

z

#lowering the case
final_english=z.lower()

#printing the text after preprocessing
final_english

#importing stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')

#printing the text after removing the stopwords
final_english

#tokenizing into sentences
sents=sent_tokenize(final_english)
sents

sents1=re.sub(r'\.',"",str(sents))

sents1

#creating the word list
words_in_list=[]
for s in sents:

  words=word_tokenize(s)
  words_in_list.append(words)
words_in_list

#replacing the dots in words_in_list
sents2=re.sub(r'\.',"",str(words_in_list))

sents2

words=word_tokenize(final_english)
words

words=[word_tokenize(w) for w in sents]
print(words)

final_english1=re.sub(r'\.',"",final_english)

words1=word_tokenize(final_english1)

words1

!pip install gensim

import gensim #importing gensim and Word2Vec
from gensim.models import Word2Vec

actuale = gensim.models.Word2Vec(size=150, min_count=1, workers=4)#training the model for english text file
actuale.build_vocab(words_in_list)
actuale.train(words_in_list, total_examples=len(words_in_list), epochs=400)

actualh = gensim.models.Word2Vec(size=150, min_count=1, workers=4)#training the model for hindi text file
actualh.build_vocab(hin_vect)
actualh.train(hin_vect, total_examples=len(hin_vect), epochs=400)

actualh.save("Actuallyh.model")#saving the model

actuale.save("Actuallye.model")#saving the model

vocabe=list(actuale.wv.vocab)#finding out the vocab for english corpus

vocabe

len(vocabe)#finding out the length of vocabulory

vocabh=list(actualh.wv.vocab)#finding out the vocab for hindi corpus

vocabh

len(vocabh)

actuale.wv.vectors#finding out the vectors

actualh.wv.most_similar("रोमांचक",topn=5)#finding out similar words

actuale.wv.most_similar("wine",topn=5)#finding out the similar words for wine

#loading the hindi english lexicon created
L2= pd.read_csv('/content/gdrive/My Drive/NLP_ASSIGNMENT4/L2.csv')

L2

L3=L2[["english_word","hindi_word","polarity"]]

L3

#creating a list of english_words in L3
words_in_L3=[]
for i in range(0,len(L3)):
  word_in_L3=L3.loc[i,"english_word"]
  words_in_L3.append(word_in_L3)
words_in_L3

len(words_in_L3)

#ENGLISH WORDS IN THE DICTIONARY 
eng_word_in_dictionary=[]
for i in range(0,len(df3)):
  eng_word=df3.loc[i,"eng"]
  eng_word_in_dictionary.append(eng_word)
len(eng_word_in_dictionary)

#HINDI WORD IN THE DICTIONARY
hindi_word_in_dictionary=[]
for i in range(0,len(df3)):
  hindi_word=df3.loc[i,"hindi"]
  hindi_word_in_dictionary.append(hindi_word)
hindi_word_in_dictionary

#Function for finding most similar word corresponding to english_word and hindi_word
def find_most_similar(english_word,hindi_word,polarity):
   most_english=actuale.wv.most_similar(english_word,topn=5)
   most_hindi=actualh.wv.most_similar(hindi_word,topn=5)
   return most_english,most_hindi

for i in range(0,500):#checking if the words in L3 belong to the vocab
  if((L3.loc[i,"english_word"] in vocabe) and (L3.loc[i,"hindi_word"] in vocabh)):
    english_word=L3.loc[i,"english_word"]
    hindi_word=L3.loc[i,"hindi_word"]
    polarity=L3.loc[i,"polarity"]
    eng,hindi=find_most_similar(english_word,hindi_word,polarity)#calling the function most_similar
    #print(i)
    #print(eng)
    #print(hindi)
    similar_hindi=[]
    similar_english=[]
    for similar1 in eng:
       similar_english.append(similar1[0])
    for similar2 in hindi:
       similar_hindi.append(similar2[0])
    #print(similar_english)
    #print(similar_hindi)
    Append_to_L2(similar_english,similar_hindi,polarity,L3)#Calling the function Append_to_L2

#function for finding the matched pairs
def Append_to_L2(english_similar,hindi_similar,polarity,L3):
  for i in range(0,len(english_similar)):
    for j in range(0,len(hindi_similar)):
      for k in range(0,len(df3)):
        if((df3.loc[k,"eng"]==english_similar[i]) and (df3.loc[k,"hindi"]==hindi_similar[j])):
          new_row = {'english_word':english_similar[i], 'hindi_word':hindi_similar[j], 'polarity':polarity}
          L3= L3.append(new_row, ignore_index=True)

L3.to_csv('L3.csv', index = True)