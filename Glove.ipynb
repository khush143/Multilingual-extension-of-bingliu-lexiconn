{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-wrIztanLtt",
        "outputId": "946c98ff-a934-4a44-c67f-7966b16cfec7"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9KL3MpqnlwU",
        "outputId": "8cd7a854-80cf-4028-974c-8799be3b4b52"
      },
      "source": [
        "!pip install glove_python"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700286 sha256=8148384b704c52cd7e4b0a3eb09a765829eb7503b81dcb0f5445b557b58a3105\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGC7qjcHpzWa",
        "outputId": "c4a6eeab-7c02-45da-f46e-9f15d9644401"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqVrP0elnN6N"
      },
      "source": [
        "#importing various libraries\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize \n",
        "import warnings\n",
        "from glove import Corpus, Glove"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh4avmDqnu5f"
      },
      "source": [
        "# function for english corpus pre-processing \n",
        "def textPreEng(text):\n",
        "    no_punct = \"\".join([i for i in text if i not in string.punctuation]) #removing punctuation\n",
        "    no_numbers=\"\".join([i for i in no_punct if i not in \"0123456789\"])  #removing numbers\n",
        "    tokens = re.split('\\W+',no_numbers) #generating tokens from sentences\n",
        "    return(tokens) # return the tokens generated "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsfDFesFn_x4"
      },
      "source": [
        "# function for hindi corpus pre-processing\n",
        "def textPreHin(text):\n",
        "    no_punct = \"\".join([i for i in text if i not in string.punctuation]) #removing punctuation\n",
        "    no_numbers=\"\".join([i for i in no_punct if i not in \"0123456789\"]) #removing numbers\n",
        "    hindi_actual=re.sub(r'[a-zA-z0-9 ]+',\" \",no_numbers) #removing english alphabets if any\n",
        "    tokens = word_tokenize(hindi_actual) # tokenizing the sentences\n",
        "    return(tokens) #return the tokens"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qeuAokJqrGE"
      },
      "source": [
        "# training the glove model on english and hindi corpora\n",
        "def glove(eng_vect,L1_eng,hin_vect,L1_hin):\n",
        "  eng_cor = Corpus() # object of english corpus\n",
        "  hin_cor = Corpus() # object of hindi corpus\n",
        "  \n",
        "  eng_cor.fit(eng_vect, window=10) #train the english vector to compute the co occurence matrix\n",
        "  hin_cor.fit(hin_vect, window=10) #train the hindi vector\n",
        "  \n",
        "  gloveE = Glove(no_components=4, learning_rate=0.04) #create a Glove object for english to create embeddings\n",
        "  gloveH = Glove(no_components=4, learning_rate=0.04) #create a Glove object for hindi to create embeddings\n",
        "\n",
        "  gloveE.fit(eng_cor.matrix, epochs=400, no_threads=5, verbose=False) #fitting the model for english vectors\n",
        "  gloveH.fit(hin_cor.matrix, epochs=400, no_threads=5, verbose=False) #fitting the model for hindi vectors\n",
        "\n",
        "  gloveE.add_dictionary(eng_cor.dictionary) #adding the english embeddings generated to the gloveE dictionary\n",
        "  gloveE.save('gloveE.model') #saving the model\n",
        "\n",
        "  gloveH.add_dictionary(hin_cor.dictionary) #adding the hindi embeddings generated to the gloveH dictionary\n",
        "  gloveH.save('gloveH.model')\n",
        "\n",
        "  # printing the embeddings for english words\n",
        "  eng_emb = []\n",
        "  for i in (eng_vect):\n",
        "    for words in i:\n",
        "      eng_emb.append(gloveE.word_vectors[gloveE.dictionary[words]])\n",
        "  print(eng_emb[:50])\n",
        "\n",
        "  # printing the embeddings for hindi words\n",
        "  hin_emb = []\n",
        "  for i in (hin_vect):\n",
        "    for words in i:\n",
        "      hin_emb.append(gloveH.word_vectors[gloveH.dictionary[words]])\n",
        "  print(hin_emb[:50])\n",
        "\n",
        "  #printing the most similar words of english words from L1\n",
        "  for i in L1_eng:\n",
        "    try:\n",
        "      print(gloveE.most_similar(i))\n",
        "      count1 = count1+1\n",
        "    except:\n",
        "      pass\n",
        "  #printing the most similar words of hindi words from L1\n",
        "  for i in L1_hin:\n",
        "     try:\n",
        "       print(gloveH.most_similar(i))\n",
        "       count1 = count1+1\n",
        "     except:\n",
        "       pass"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Pb9ipOqvBn"
      },
      "source": [
        "# loading the english corpus to csv file and performing pre-processing on it\n",
        "eng_corpus = pd.read_csv(r'/content/gdrive/My Drive/NLP_ASSIGNMENT4/english.txt', sep = \"\\n\",encoding=\"utf8\",header = None)\n",
        "eng_corpus.columns = ['sentences']\n",
        "eng_corpus['clean text'] = eng_corpus['sentences'].apply(lambda t: textPreEng(t.lower()))\n",
        "df = eng_corpus['clean text']\n",
        "eng_vect = df.values.tolist() # converting the tokens into list of lists\n",
        "\n",
        "# Loading the L1 csv file created in step 1 and converting the words into list\n",
        "L1 = pd.read_csv(r'/content/gdrive/My Drive/NLP_ASSIGNMENT4/L1.csv')\n",
        "df1 = L1['english_word']\n",
        "L1_eng = df1.values.tolist()\n",
        "\n",
        "L1 = pd.read_csv(r'/content/gdrive/My Drive/NLP_ASSIGNMENT4/L1.csv')\n",
        "df2 = L1['hindi_word']\n",
        "L1_hin = df2.values.tolist()\n",
        "\n",
        "# loading the hindi corpus to csv file and performing pre-processing on it\n",
        "hin_corpus = pd.read_csv (r'/content/gdrive/My Drive/NLP_ASSIGNMENT4/hindi.txt', sep = \"\\n\",encoding=\"utf8\",header = None)\n",
        "hin_corpus.columns = ['sentences']\n",
        "hin_corpus['clean text'] = hin_corpus['sentences'].apply(lambda t: textPreHin(t))\n",
        "df = hin_corpus['clean text']\n",
        "hin_vect = df.values.tolist() # converting the tokens into list of lists\n",
        "\n",
        "# calling the glove function\n",
        "glove(eng_vect,L1_eng,hin_vect,L1_hin)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}